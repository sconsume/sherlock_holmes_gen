{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "016168a8",
   "metadata": {},
   "source": [
    "<h1><center> Victorian-style (Holmesian) Passage Creation </center><h1>\n",
    "<h1><center> Jesus Tello </center></h1>\n",
    "<h1><center> June 2025 </center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee4dcf5",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "In this project, we explore character-level recurrent neural network (char-RNN) modeling of Arthur Conan Doyle’s Sherlock Holmes complete story using the char-rnn-tensorflow implementation by Sherjil Ozair<sup>1</sup>. We first create a baseline Long Short-Term Memory (LSTM) model (2 layers, 128 units, sequence length 50) trained on the full Sherlock Holmes corpus. Observing suboptimal coherence in generated samples, we continually tune hyperparameters hidden size, sequence length, dropout, learning rate, decay rate, and number of epochs to improve text quality and convergence speed. We then compare LSTM against alternative RNN cells (Neural Architecture Search, NAS) under matched configurations, conducting a final learning-rate sweep on the best-performing variant. The results show that the NAS cell achieves lower training loss and generates more coherent, context-aware Victorian-style text than the baseline LSTM. This report details our dataset, methodology, experiment workflow, hyperparameter choices, and analysis of generated samples and training loss curves.\n",
    "\n",
    "## Introduction\n",
    "The goal of this project is to show that a simple character-level recurrent neural network (char-RNN) can learn the style of Victorian prose from Arthur Conan Doyle’s Sherlock Holmes stories and generate new passages on demand. By processing text one character at a time and maintaining an evolving hidden state, the model captures syntax, punctuation, and narrative context. A successful char‑RNN opens up possibilities for creative writing tools that can suggest or extend text in an arbitrarily chosen or given style.\n",
    "\n",
    "### Background on RNNs\n",
    "RNNs work by carrying a hidden state through a sequence of inputs. At each character, the network takes the current character embedding plus the previous hidden state, applies a learned update, and produces a new hidden state (and, when training, a prediction). LSTM and GRU cells add simple gating mechanisms - learned switches that control what to remember or forget - to help capture patterns over longer spans. NAS cells go further by automatically searching for the best gate structure while training on the data. <sup>1</sup>\n",
    "## Methodology\n",
    "### Dataset\n",
    "Given that the instructions were provided to do so on the GitHub repository <sup>2</sup>, we opted to download the complete, ASCII texts of all Sherlock Holmes novels and short stories from the public domain repository [sherlock-holm.es/ascii/](https://sherlock-holm.es/stories/plain-text/cnus.txt). The raw text (~3 MB) was used directly - no tokenization, title removal, or preprocessing on this raw text data was performed. Perhaps removing the gaps of text between chapters or short stories, and removing the title names would have improved the results of the model sample text generation, however it did not seem necessary. There is such an abundance of text training data, removing the titles and relatively small amount of newline text gaps would not have made much of a difference in training. As for reading the data, a Python TextLoader splits the text into sequences of fixed length (`--seq_length`) and batches (`--batch_size`).\n",
    "### RNN Implementation \n",
    "The implementation of the character-level recurrent neural network was given from Sherjil Ozair's public GitHub repository, utilizing a two-script structure in the following format:\n",
    "- Argument parsing: At the top of train.py, an ArgumentParser defines flags for data paths (`--data_dir`, `--save_dir`, `--log_dir`), checkpointing (`--save_every`, `--init_from`), model choice (`--model`), architecture (`--rnn_size`, `--num_layers`), and optimization (`--seq_length`, `--batch_size`, `--num_epochs`, `--learning_rate`, `--decay_rate`, `--grad_clip`, `--input_keep_prob`, `--output_keep_prob`). The default values of these parameters can be overridden, and their defaults are provided in train.py.\n",
    "- train(args) function:\n",
    "    - Data loading: Instantiates TextLoader(`args.data_dir`, `args.batch_size`, `args.seq_length`), which reads input.txt, constructs a vocabulary, and creates iterable batches of character indices.\n",
    "- Model creation: \n",
    "    - The Model class in `model.py` constructs a stack of RNN cells (LSTM/GRU/NAS) wrapped with dropout as specified. It embeds input character indices into dense vectors, unrolls them through the multi-layer RNN, and projects outputs back to the character vocabulary via a linear (softmax) layer.\n",
    "- Training loop:\n",
    "    - Furthermore, train(args) instantiates TensorBoard summaries and a `Saver` for checkpoints, as well as initializes or restores model weights. For each epoch, it decays the learning rate, resets the data pointer, and loops over batches\n",
    "        - Also fetches input/target pairs, runs a single `sess.run` to compute loss, update state, and apply gradients with clipping\n",
    "        - As well as logs loss summaries, prints progress, and saves checkpoints at given intervals. \n",
    "- Sampling:\n",
    "    - The `Model.sample` method initializes a single-step state, “primes” the RNN with a short prompt, then iteratively feeds back sampled or greedy predictions to generate new characters one at a time.\n",
    "\n",
    "The design of this implementation makes clean and separate input/outputs, model definitions for training, and easy inference. Swapping cell types (model types) is straightforward, and tuning hyperparameters is also extremely easy.\n",
    "### Baseline Model and Hyperparameters Descriptions\n",
    "The baseline LSTM model parameters were suggested, and given by Ozair in the GitHub repository<sup>2</sup>.\n",
    "- Hidden units (`--rnn_size`): 128\n",
    "- Layers (`--num_layers`): 2\n",
    "- Sequence length (`--seq_length`): 50\n",
    "- Batch size (`--batch_size`): 50\n",
    "- Learning rate (`--learning_rate`): 0.002\\\n",
    "- Decay rate (`--decay_rate`): 0.97\n",
    "- Input Keep Probability (`NULL`): Left default at 1.0\n",
    "- Output Keep Probability (`NULL`): Left default at 1.0\n",
    "- Gradient clipping (`--grad_clip`): 5.0\n",
    "- Epochs (`--num_epochs`): 20\n",
    "Below are brief descriptions for what each parameter entails:\n",
    "- Hidden units (`--rnn_size`): Size of each RNN cell’s state vector. Larger values increase capacity but require more computation and data.\n",
    "- Layers (`--num_layers`): Number of RNN layers stacked. More layers can capture hierarchical patterns but may overfit or be harder to train.\n",
    "- Sequence length (`--seq_length`): Number of characters the network sees before truncated backpropagation. Controls context window size vs. memory requirements.\n",
    "- Batch size (`--batch_size`): Number of sequences processed in parallel. Impacts training stability and GPU utilization.\n",
    "- Learning rate (`--learning_rate`): Initial step size and its per-epoch decay factor. Higher rates speed learning but risk instability.\n",
    "- Decay rate (`--decay_rate`): Fraction of units retained during training to regularize and prevent overfitting.\n",
    "- Input Keep Probability (`--input_keep_prob`) : The probability of keeping weights in the hidden layer.\n",
    "- Output Keep Probability (`--output_keep_prob`): The probability of keeping weights in the input layer.\n",
    "- Gradient clipping (`--grad_clip`): Caps gradient norm to avoid exploding gradients in deep RNNs.\n",
    "- Epochs (`--num_epochs`): Complete passes over data. More epochs can improve fit but may overfit.\n",
    "\n",
    "Given the descriptions of these parameters and their meaning, the only parameter I selectively chose prior to knowing much about the input data, model creation and text sample generation, was sequence length. As a general rule of thumb, most sentences in writing, speech, conversations, etc. are about 50 characters long. As such, this value was the only one personally selected by me for the initial baseline. \n",
    "### Hyperparameter Tuning\n",
    "We ended up branching off the baseline model in multiple ways after obtaining initial results. Below are the various configurations tested. \n",
    "\n",
    "<div class=\"table-container\">\n",
    "  <table>\n",
    "    <thead>\n",
    "      <tr>\n",
    "        <th>Run Name</th>\n",
    "        <th>--num_layers</th>\n",
    "        <th>--rnn_size</th>\n",
    "        <th>--seq_length</th>\n",
    "        <th>--batch_size</th>\n",
    "        <th>--num_epochs</th>\n",
    "        <th>--learning_rate</th>\n",
    "        <th>--decay_rate</th>\n",
    "        <th>--input_keep_prob</th>\n",
    "        <th>--output_keep_prob</th>\n",
    "        <th>--grad_clip</th>\n",
    "      </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "      <tr>\n",
    "        <td>updated_run_lstm</td>\n",
    "        <td>2</td>\n",
    "        <td><strong>256</strong></td>\n",
    "        <td><strong>64</strong></td>\n",
    "        <td>50</td>\n",
    "        <td><strong>10</strong></td>\n",
    "        <td><strong>0.001</strong></td>\n",
    "        <td><strong>0.98</strong></td>\n",
    "        <td><strong>0.8</strong></td>\n",
    "        <td><strong>0.8</strong></td>\n",
    "        <td>5.0</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td>updated_run_2_lstm</td>\n",
    "        <td><strong>3</strong></td>\n",
    "        <td>256</td>\n",
    "        <td><strong>100</strong></td>\n",
    "        <td><strong>80</strong></td>\n",
    "        <td><strong>5</strong></td>\n",
    "        <td><strong>0.005</strong></td>\n",
    "        <td>0.98</td>\n",
    "        <td><strong>0.6</strong></td>\n",
    "        <td><strong>0.6</strong></td>\n",
    "        <td>5.0</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td>lr_0001_lstm</td>\n",
    "        <td>3</td>\n",
    "        <td>256</td>\n",
    "        <td><strong>64</strong></td>\n",
    "        <td><strong>50</strong></td>\n",
    "        <td><strong>8</strong></td>\n",
    "        <td><strong>0.001</strong></td>\n",
    "        <td><strong>0.97</strong></td>\n",
    "        <td><strong>0.8</strong></td>\n",
    "        <td><strong>0.8</strong></td>\n",
    "        <td>5.0</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td>lr_0002_lstm</td>\n",
    "        <td>3</td>\n",
    "        <td>256</td>\n",
    "        <td>64</td>\n",
    "        <td>50</td>\n",
    "        <td>8</td>\n",
    "        <td><strong>0.002</strong></td>\n",
    "        <td>0.97</td>\n",
    "        <td>0.8</td>\n",
    "        <td>0.8</td>\n",
    "        <td>5.0</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td>lr_0003_lstm</td>\n",
    "        <td>3</td>\n",
    "        <td>256</td>\n",
    "        <td>64</td>\n",
    "        <td>50</td>\n",
    "        <td>8</td>\n",
    "        <td><strong>0.003</strong></td>\n",
    "        <td>0.97</td>\n",
    "        <td>0.8</td>\n",
    "        <td>0.8</td>\n",
    "        <td>5.0</td>\n",
    "      </tr>\n",
    "    </tbody>\n",
    "  </table>\n",
    "</div>\n",
    "\n",
    "</body>\n",
    "</html>\n",
    "\n",
    "### Model Comparison\n",
    "After hyperparameter-tuning the LSTM model, we reran all the previously done hyperparameters on a NAS model, and opted to compare the train-loss and sample text generation between the two models. Below is a table containing all the hyperparameters once again, performed on the NAS model.\n",
    "\n",
    "<div class=\"table-container\">\n",
    "  <table>\n",
    "    <thead>\n",
    "      <tr>\n",
    "        <th>Run Name</th>\n",
    "        <th>--num_layers</th>\n",
    "        <th>--rnn_size</th>\n",
    "        <th>--seq_length</th>\n",
    "        <th>--batch_size</th>\n",
    "        <th>--num_epochs</th>\n",
    "        <th>--learning_rate</th>\n",
    "        <th>--decay_rate</th>\n",
    "        <th>--input_keep_prob</th>\n",
    "        <th>--output_keep_prob</th>\n",
    "        <th>--grad_clip</th>\n",
    "      </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "      <tr>\n",
    "        <td>baseline_nas</td>\n",
    "        <td>2</td>\n",
    "        <td>128</td>\n",
    "        <td>50</td>\n",
    "        <td>50</td>\n",
    "        <td>20</td>\n",
    "        <td>0.002</td>\n",
    "        <td>0.97</td>\n",
    "        <td>1.0</td>\n",
    "        <td>1.0</td>\n",
    "        <td>5.0</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td>updated_run_nas</td>\n",
    "        <td>2</td>\n",
    "        <td><strong>256</strong></td>\n",
    "        <td><strong>64</strong></td>\n",
    "        <td>50</td>\n",
    "        <td><strong>10</strong></td>\n",
    "        <td><strong>0.001</strong></td>\n",
    "        <td><strong>0.98</strong></td>\n",
    "        <td><strong>0.8</strong></td>\n",
    "        <td><strong>0.8</strong></td>\n",
    "        <td>5.0</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td>updated_run_2_nas</td>\n",
    "        <td><strong>3</strong></td>\n",
    "        <td>256</td>\n",
    "        <td><strong>100</strong></td>\n",
    "        <td><strong>80</strong></td>\n",
    "        <td><strong>5</strong></td>\n",
    "        <td><strong>0.005</strong></td>\n",
    "        <td>0.98</td>\n",
    "        <td><strong>0.6</strong></td>\n",
    "        <td><strong>0.6</strong></td>\n",
    "        <td>5.0</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td>lr_0001_nas</td>\n",
    "        <td>3</td>\n",
    "        <td>256</td>\n",
    "        <td><strong>64</strong></td>\n",
    "        <td><strong>50</strong></td>\n",
    "        <td><strong>8</strong></td>\n",
    "        <td><strong>0.001</strong></td>\n",
    "        <td><strong>0.97</strong></td>\n",
    "        <td><strong>0.8</strong></td>\n",
    "        <td><strong>0.8</strong></td>\n",
    "        <td>5.0</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td>lr_0002_nas</td>\n",
    "        <td>3</td>\n",
    "        <td>256</td>\n",
    "        <td>64</td>\n",
    "        <td>50</td>\n",
    "        <td>8</td>\n",
    "        <td><strong>0.002</strong></td>\n",
    "        <td>0.97</td>\n",
    "        <td>0.8</td>\n",
    "        <td>0.8</td>\n",
    "        <td>5.0</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td>lr_0003_nas</td>\n",
    "        <td>3</td>\n",
    "        <td>256</td>\n",
    "        <td>64</td>\n",
    "        <td>50</td>\n",
    "        <td>8</td>\n",
    "        <td><strong>0.003</strong></td>\n",
    "        <td>0.97</td>\n",
    "        <td>0.8</td>\n",
    "        <td>0.8</td>\n",
    "        <td>5.0</td>\n",
    "      </tr>\n",
    "    </tbody>\n",
    "  </table>\n",
    "</div>\n",
    "\n",
    "### Model Evaluation\n",
    "Each and every model was quantitatively and qualitatively evaluted, by extracted `train_loss` from TensorBoard event files to plot convergence curves, and by generating 300-character samples with a fixed prompt, initially primed by \"I ,\" and later primed by default \"The .\" The choice for initial prime \"I \" was to see if the char-RNN could capture the unique character of Holmes himself - to view if any conversations or dialogue between characters could be captured. Some models were able to do so, and others weren't. The coherence, sentence structure, and context of each generated sample from each model was evaluated and led to the various hyper-parameter changes.\n",
    "\n",
    "## Results\n",
    "The training loss curves of all models, displayed below: \n",
    "\n",
    "<img src=\"output.png\" alt=\"All Train Loss Curves\">\n",
    "\n",
    "\n",
    "Below are the final train loss values for each of the models:\n",
    "\n",
    "<table>\n",
    "  <caption>Final Train Loss</caption>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>Model</th>\n",
    "      <th>Variant</th>\n",
    "      <th>LSTM</th>\n",
    "      <th>NAS</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>1</td>\n",
    "      <td>baseline</td>\n",
    "      <td>1.155074</td>\n",
    "      <td>1.099551</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>2</td>\n",
    "      <td>lr_0001</td>\n",
    "      <td>1.258781</td>\n",
    "      <td>1.2311153</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>3</td>\n",
    "      <td>lr_0002</td>\n",
    "      <td>1.174434</td>\n",
    "      <td>1.164366</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>4</td>\n",
    "      <td>lr_0003</td>\n",
    "      <td>1.218899</td>\n",
    "      <td>1.181866</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>5</td>\n",
    "      <td>updated_run</td>\n",
    "      <td>1.188411</td>\n",
    "      <td>1.188163</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>6</td>\n",
    "      <td>updated_run_2</td>\n",
    "      <td>1.546502</td>\n",
    "      <td>1.502885</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "Below are the sample text generated from both the LSTM and NAS models:\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <caption>Sample Text Comparison</caption>\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th>model</th>\n",
    "      <th>variant</th>\n",
    "      <th>lstm_sample</th>\n",
    "      <th>nas_sample</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>1</td>\n",
    "      <td>baseline</td>\n",
    "      <td>The when a consults, follows. <br>\n",
    "      <br>\n",
    "          \"Did they trouble--more to you to give a quickle. His rust the <br> part of \"I should do your world: 'He'll not? I will reach of <br>\n",
    "          that it is reach coloured down to this eye to my father incush <br>\n",
    "          which I have had taken--Charpens to long-beaous confess <br>\n",
    "          of hi</td>\n",
    "      <td> The was the <br>\n",
    "     deduction to have none. That is that I had have hold rather than I cut to <br>\n",
    "     read that. <br> <br>\n",
    "     \"I belp up me conceals appear of poor opportunity. She become larger man, we intrusibly <br>\n",
    "     were one should produced himself, \"leaving the facts.\" <br> <br>\n",
    "     \"I have didn't you were stood behind\n",
    "    </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>2</td>\n",
    "      <td>updated_run</td>\n",
    "      <td>The Dure, and lyon <br>\n",
    "     was no word dropped upon; The person had very brink to a past. <br> <br>\n",
    "     It was a friend it has my moulder appeared to know in cause, and the langers <br> \n",
    "     definitely with home of its visitor. I had professed notice, <br>\n",
    "     I should get open upon the turn. As the dent of him, has some t</td>\n",
    "      <td>The we shall <br>\n",
    "     call the floor which came out, and that's carriage. And so you use it <br>\n",
    "     with a rasp, between the bourdering garden. Lestrade?\" <br> <br>\n",
    "     \"He has too veaying my throat of his knee in a table, however, I have <br>\n",
    "     looked intending to you to the else aloud some of my life. It is in the</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>3</td>\n",
    "      <td>updated_run_2</td>\n",
    "      <td>The had gever matter of Minst mrown his pict-repare-to dad dright Caich <br>\n",
    "     cary <br> \n",
    "     very few thiths of up to what our trifial.\" <br> <br>\n",
    "      Before is it to have seen by <br> \n",
    "     none it, but had work.\" <br> <br>\n",
    "     \"And to he give, this horant dacket of the way.\" I conclise ot an window, <br>\n",
    "     the lad. <br> <br>\n",
    "     \"How!\"</td>\n",
    "      <td>The Hals. That is now protess, there was not any <br>\n",
    "     words. When I am seen then we shall feet me further. \"It was pure him by from Deal his tapian women <br>\n",
    "     mousing off all guess miss become.\" <br> <br>\n",
    "     \"I must will both malast. We know the rooms was recy?\" <br> <br>\n",
    "     \"Well, as So, what dishoised in the looks <br> </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>4</td>\n",
    "      <td>lr_0001</td>\n",
    "      <td>The thing <br>\n",
    "     he was Daken Street. I dure made a case from him., a black and a back <br>\n",
    "     Room on the Lair, the hill to gray--I saw no carefully our lady appeans <br>\n",
    "     make? We will, he will pite with the letter. Let kist Last then, heaven's <br>\n",
    "     prisent, and the ampering gore-bay widel in the light upon</td>\n",
    "      <td>The buldy. It is the <br>\n",
    "     unfortunate his infidence. It was more never <br>\n",
    "     after Berdy, it are, the two pluttering than who paused all to <br>\n",
    "     not in sequent wife when I get him to deep in my trouble. He was spoke <br>\n",
    "     fits it. Certainly; but was the man with in suiting satisficate months. <br> <br>\n",
    "     \"There</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>5</td>\n",
    "      <td>lr_0002</td>\n",
    "      <td>The Stable. She <br>\n",
    "     could go out in Cardias.  I've glanced to see West's eyes as meet, you'll <br>\n",
    "     not pursuit them by seeing you a satisfactive demente of <br>\n",
    "     none and wanted cause in a hurard, and he should person along the <br>\n",
    "     'emanly.  These as every oldime still chamber had needed to me find my</td>\n",
    "      <td>The again <br>\n",
    "     cizarys of the one were began to a morning after a street evident <br>\n",
    "     as pointed him on. <br>\n",
    "     This gun of bright among it as to a biolent outing lantern of <br>\n",
    "     the step-parain, which over for us for Ga, and the omized of the <br>\n",
    "     deed by good-noted in his face which is interviewing rou</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>6</td>\n",
    "      <td>lr_0003</td>\n",
    "      <td>The young assuther <br>\n",
    "     at the son's murderer. Thank go informated my task's bottom,\" said <br>\n",
    "     the huge, geting the other victim, now, but does ever! I <br>\n",
    "     could expect. Of course, with the morning, only opening her <br>\n",
    "     all stair of the strange man whose rivent ended by the house of light. The coolle</td>\n",
    "      <td>The may <br>\n",
    "     tel not kitchen. It is courted, but from Holmes,\" said he, criminals, and <br>\n",
    "     a place and aslarental, clear that he turned and secured, which <br>\n",
    "     seemed out of the natural very line. <br> <br>\n",
    "     \"And I have have to have had our dear?\" <br> <br>\n",
    "     Why drew him go hards at London, or his mind which wal</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "## Discussion\n",
    "The training curves (Figure above) and final loss table reveal a consistent pattern: across all hyperparameter settings, the NAS cell attains a lower cross-entropy than its LSTM counterpart. For the baseline configuration, NAS converges to ~1.10 versus LSTM’s ~1.16, indicating a smoother loss landscape and more efficient gradient propagation. In our tuned experiments, the updated_run variants demonstrate nearly identical final losses (~1.188) for both LSTM and NAS, suggesting that certain hyperparameter choices (e.g. increased hidden size, dropout) can close the gap between architectures. However, under the more aggressive settings of updated_run_2, NAS again outperforms LSTM (1.50 vs. 1.55), consistent with its learned gating structure that better handles longer sequences (seq_length=100) and higher dropout rates (0.6).\n",
    "\n",
    "In the final three models for each model type respectively, NAS also obtains a smaller final training loss value, at lr=0.001 (1.23 vs. 1.26), lr=0.002 (1.16 vs. 1.17), and lr=0.003 (1.18 vs. 1.22). However, obtaining a lower training loss doesn't necessarily guarantee that the generated sample text will be readable. \n",
    "\n",
    "The most striking observation is that simply swapping from an LSTM cell to NAS under identical settings greatly reduces the frequency of nonsensical “words” and encourages more valid English patterns - even at modest capacity (2 layers, 128 units, sequence length 50). Increasing the hidden‐state size to 256 and modestly extending context (sequence length 64) yields real character names and partial sentence structure, but it is only when we stack three layers, stretch the context window to 100 characters, and apply moderate dropout (keep probability 0.6) that both LTSM and NAS  produce sentence fragments with proper punctuation. However, the LSTM variant for model 3, though has relatively accuracte sentence structure and syntax, has a bunch of nonsensical made-up words. The NAS variant of this model was actually able to, under these parameter conditions, strings together somewhat logical clauses (“That is now protest, there was not any words. When I am seen then we shall feet me further.”), and frames short dialogues, whereas the LSTM counterpart still stumbles over invented tokens and fractured syntax. Learning‐rate sweeps around 0.001–0.002 further refine the balance between stability and novelty: too low a rate produces overly safe, repetitive text, while too high a rate reintroduces spelling errors. Taken together, the best‐performing sample - “`updated_run_2_nas`” - demonstrates that deeper stacking, longer context, moderate dropout, and the NAS cell’s automated gating synergy are key to generating the most Holmesian‐style prose with both creativity and readability. Additionally, it proves that achieving the lowest final training loss isn't everything - as `updated_run_2_nas` boasted the second highest final training loss value.\n",
    "\n",
    "## Conclusion \n",
    "Through careful hyperparameter tuning and by comparing LSTM and NAS cells, we showed that deeper networks with moderate dropout and longer context windows produce the most coherent Sherlock-style text. The best model we obtained, a 3-layer, 256-unit NAS rnn with a sequence length of 100 and keep-probabilities of 0.6, even had one of the higher final training loss values, but still created the most syntax-correct, somewhat context-logical mimicry of Sherlock Holmes text. Though, we were unable to completely and accurately produce perfect Holmesian sample sentences, there is definitely room for improvement. Initially though to not cause problems, working with the raw, unprocessed text data, without removing titles and the various newline characters caused a lot of them to appear in our sample text generations. Future work should clean the text. Experimenting with even more hyperparamter tuning, paying attention to sequence length, inner and outer keep probability, rnn_size, and num_layers might prove to improve text generation.\n",
    "\n",
    "## References\n",
    "References\n",
    "\n",
    "1. Karpathy, A. (2015). The Unreasonable Effectiveness of Recurrent Neural Networks. The unreasonable effectiveness of recurrent neural networks. http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "\n",
    "2. Ozair, S. (2016). char-rnn-tensorflow. GitHub. [https://github.com/sherjilozair/char-rnn-tensorflow](https://github.com/sherjilozair/char-rnn-tensorflow)\n",
    "\n",
    "3. The Complete Sherlock Holmes. The complete Sherlock Holmes. (n.d.). https://sherlock-holm.es/stories/plain-text/cnus.tx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec72574",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
